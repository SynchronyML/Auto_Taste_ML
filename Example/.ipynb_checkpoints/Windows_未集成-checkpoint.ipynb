{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31d68cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎来到Czy的快乐星球\n"
     ]
    }
   ],
   "source": [
    "# 开始加载环境\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 从这里开始绘制箱线图表示误差的大小\n",
    "from pandas.plotting import andrews_curves\n",
    "\n",
    "# 忽略烦人的红色提示\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,train_test_split,cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,average_precision_score\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# 自定义颜色\n",
    "import matplotlib\n",
    "import matplotlib.colors as col\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 欢饮语\n",
    "print(\"欢迎来到Czy的快乐星球\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea1743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试数据\n",
    "df = pd.read_csv(\"2_data_deal_smote.csv\")\n",
    "X = df.iloc[:,:-1]\n",
    "Y = df[\"label\"]\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b0dd4",
   "metadata": {},
   "source": [
    "# LogisticRegressionCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45aa7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'none', 'solver': 'liblinear'} 0.7574333800841514 LogisticRegressionCV(class_weight='none', max_iter=10000, n_jobs=-1,\n",
      "                     random_state=0, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegressionCV, Perceptron\n",
    "def LogisticRegressionCV_mdoel(X, Y, cv, score):\n",
    "    LogisticRegressionCV_auto = LogisticRegressionCV(random_state=0,\n",
    "                                                     n_jobs=-1,\n",
    "                                                     max_iter=10000,penalty='l2')\n",
    " \n",
    "    # 搜索的参数是para\n",
    "    param_dict = {\n",
    "        \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        \"class_weight\": ['none', 'balanced'],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=LogisticRegressionCV_auto,\n",
    "                             param_grid=param_dict,\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2, LogisticRegressionCV_auto = LogisticRegressionCV_mdoel(\n",
    "    X, Y, cv, score)\n",
    "print(LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2,\n",
    "      LogisticRegressionCV_auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a87738",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59c9754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average': False, 'early_stopping': False, 'fit_intercept': True, 'loss': 'modified_huber', 'penalty': 'l1', 'warm_start': True} 0.72945301542777 SGDClassifier(class_weight='balanced', loss='modified_huber', max_iter=10000,\n",
      "              n_jobs=-1, penalty='l1', random_state=0, warm_start=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "def SGDClassifier_model(X, Y, cv, score):\n",
    "\n",
    "    # 首先实例化各个回归方法\n",
    "    SGDClassifier_auto = SGDClassifier(random_state=0,n_jobs=-1,max_iter=10000,class_weight='balanced')\n",
    "    param_dict = {\"penalty\":['l2','elasticnet','l1'],  #\n",
    "                  \"loss\":[ 'modified_huber', 'log'], #'hinge'\n",
    "                  \"average\":[True,False],\n",
    "                  \"warm_start\":[True,False],\n",
    "                  \"early_stopping\":[True,False],\n",
    "                  \"fit_intercept\":[True,False],\n",
    "                 }\n",
    "    estimator = GridSearchCV(estimator = SGDClassifier_auto, param_grid=param_dict, cv=cv,n_jobs=-1,scoring = score)\n",
    "    estimator.fit(X,Y)\n",
    "    return estimator.best_params_,estimator.best_score_,estimator.best_estimator_\n",
    "\n",
    "\n",
    "SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto = SGDClassifier_model(X, Y, cv, score)\n",
    "print(SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d27646",
   "metadata": {},
   "source": [
    "# LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "359fb3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd', 'store_covariance': True} 0.7033660589060309 LinearDiscriminantAnalysis(store_covariance=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def LinearDiscriminantAnalysis_model(X, Y, cv, score):\n",
    "\n",
    "    # 首先实例化各个判别分析方法\n",
    "    LinearDiscriminantAnalysis_auto = LinearDiscriminantAnalysis()\n",
    "    param_dict = {\n",
    "        \"solver\": ['svd', 'lsqr'],  #, 'eign' 不可以用\n",
    "        \"store_covariance\": [True, False],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=LinearDiscriminantAnalysis_auto,\n",
    "                             param_grid=param_dict,\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "LinearDiscriminantAnalysis_auto_tmp_1, LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto = LinearDiscriminantAnalysis_model(\n",
    "    X, Y, cv, score)\n",
    "print(LinearDiscriminantAnalysis_auto_tmp_1,\n",
    "      LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228001e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2424c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# def LinearSVC_model(X, Y, cv, score):  # 这就是感知机\n",
    "#     # 首先实例化各个回归方法\n",
    "#     LinearSVC_auto = LinearSVC(random_state=0, max_iter=10000,penalty='l2')\n",
    "#     param_dict = {\n",
    "#         \"loss\": ['hinge', 'squared_hinge'],\n",
    "# #         \"dual\": [True, False],\n",
    "# #         \"fit_intercept\": [True, False],\n",
    "# #         \"class_weight\": ['none', 'balanced'],\n",
    "#     }\n",
    "#     estimator = GridSearchCV(estimator=LinearSVC_auto,\n",
    "#                              param_grid=param_dict,\n",
    "#                              cv=cv,\n",
    "#                              n_jobs=-1,\n",
    "#                              scoring=score)\n",
    "#     estimator.fit(X, Y)\n",
    "#     return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "# LinearSVC_auto_tmp_1, LinearSVC_auto_tmp_2, LinearSVC_auto = LinearSVC_model(\n",
    "#     X, Y, cv, score)\n",
    "# print(LinearSVC_auto_tmp_1, LinearSVC_auto_tmp_2, LinearSVC_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b5873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ada4c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 'scale', 'kernel': 'linear', 'shrinking': True} 0.7250818139317439 SVC(class_weight='balanced', kernel='linear', probability=True, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "def SVC_model(X, Y, cv, score):  # 这就是感知机\n",
    "    # 首先实例化各个回归方法\n",
    "    SVC_auto = SVC(random_state=0, probability=True,class_weight='balanced')\n",
    "    param_dict = {\n",
    "        \"gamma\": ['scale', 'auto'],\n",
    "        \"kernel\": ['linear', 'rbf', 'sigmoid'],  # 'precomputed' 这个参数对于X有额外的要求,取消 , 'poly' 主要是因为算起来太慢了\n",
    "        \"shrinking\": [True, False],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=SVC_auto, param_grid=param_dict, cv=cv, n_jobs=-1, scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto = SVC_model(X, Y, cv, score)\n",
    "print(SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ba652bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_features': 'auto', 'splitter': 'best'} 0.9004207573632538 DecisionTreeClassifier(class_weight='balanced', max_features='auto',\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def DecisionTreeClassifier_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    DecisionTreeClassifier_auto = DecisionTreeClassifier(\n",
    "        random_state=0, class_weight='balanced')\n",
    "    param_dict = {\n",
    "        \"criterion\": ['gini', 'entropy'],\n",
    "        \"splitter\": ['best', 'random'],\n",
    "        \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=DecisionTreeClassifier_auto,\n",
    "                             param_grid=param_dict,\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2, DecisionTreeClassifier_auto = DecisionTreeClassifier_model(\n",
    "    X, Y, cv, score)\n",
    "print(DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2,\n",
    "      DecisionTreeClassifier_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "841ed498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R'} 0.8984104721832631 AdaBoostClassifier(random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTreeClassifier_auto_ada = DecisionTreeClassifier_auto\n",
    "def AdaBoostClassifier_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    AdaBoostClassifier_auto = AdaBoostClassifier(random_state=0)\n",
    "    param_dict = {\n",
    "                 \"algorithm\":['SAMME', 'SAMME.R']}\n",
    "    estimator = GridSearchCV(estimator = AdaBoostClassifier_auto, param_grid=param_dict, cv=cv,n_jobs=-1,scoring =score)\n",
    "    estimator.fit(X,Y)\n",
    "    return estimator.best_params_,estimator.best_score_,estimator.best_estimator_\n",
    "\n",
    "AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto = AdaBoostClassifier_model(X, Y, cv, score)\n",
    "print(AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4a02e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': 'True', 'bootstrap_features': 'True'} 0.8876344086021504 BaggingClassifier(bootstrap='True', bootstrap_features='True', n_jobs=-1,\n",
      "                  random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "def BaggingClassifier_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    BaggingClassifier_auto = BaggingClassifier(random_state=0, n_jobs=-1)\n",
    "    param_dict = {  # \"n_estimators\":[10,30,50,70,90,110],\n",
    "        \"bootstrap\": [\"True\", \"False\"],\n",
    "        \"bootstrap_features\": [\"True\", \"False\"],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=BaggingClassifier_auto,\n",
    "                             param_grid=param_dict,\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2, BaggingClassifier_auto = BaggingClassifier_model(\n",
    "    X, Y, cv, score)\n",
    "print(BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2,\n",
    "      BaggingClassifier_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e7ce830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'loss': 'exponential', 'max_features': 'sqrt'} 0.9134174848059841 GradientBoostingClassifier(loss='exponential', max_features='sqrt',\n",
      "                           random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "def GradientBoostingClassifier_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    GradientBoostingClassifier_auto = GradientBoostingClassifier(random_state=0)\n",
    "    param_dict = {\"loss\": [\"deviance\", \"exponential\"],\n",
    "                  #                   \"n_estimators\":[10,30,50,70,90,110],\n",
    "                  #                  \"learning_rate\":[0,0.5,1,1.5,2],\n",
    "                  \"criterion\": ['friedman_mse', 'mse', \"mae\"],\n",
    "                  \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
    "    estimator = GridSearchCV(estimator=GradientBoostingClassifier_auto, param_grid=param_dict, cv=cv, n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto = GradientBoostingClassifier_model(X, Y, cv, score)\n",
    "print(GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "605bc2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': 'True', 'criterion': 'gini', 'max_features': 'auto', 'oob_score': 'True'} 0.9221131369798972 RandomForestClassifier(bootstrap='True', class_weight='balanced', n_jobs=-1,\n",
      "                       oob_score='True', random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForestClassifier_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    RandomForestClassifier_auto = RandomForestClassifier(\n",
    "        random_state=0, n_jobs=-1, class_weight=\"balanced\")\n",
    "    param_dict = {\n",
    "        #                   \"n_estimators\":[10,30,50,70,90,110],\n",
    "        \"criterion\": ['gini', 'entropy'],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        #         \"\": ['none', 'balanced', \"balanced_subsample\"],\n",
    "        \"bootstrap\": [\"True\", \"False\"],\n",
    "        \"oob_score\": [\"True\", \"False\"],\n",
    "    }\n",
    "    estimator = GridSearchCV(estimator=RandomForestClassifier_auto,\n",
    "                             param_grid=param_dict,\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2, RandomForestClassifier_auto = RandomForestClassifier_model(\n",
    "    X, Y, cv, score)\n",
    "print(RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2,\n",
    "      RandomForestClassifier_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13551310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from xgboost import plot_importance\n",
    "# def xgboost_model(X, Y, cv, score):\n",
    "\n",
    "#     xgboost_auto = xgb.XGBClassifier(random_state=0, n_jobs=1)\n",
    "#     param_dict = {\"maximize\": [\"True\", \"False\"],\n",
    "#                   \"objective\": ['binary:logistic','binary:hinge'],  # reg:linear回归时使用\n",
    "#                   }\n",
    "#     estimator = GridSearchCV(estimator=xgboost_auto, param_grid=param_dict, cv=cv, n_jobs=-1, scoring=score)\n",
    "#     estimator.fit(X, Y)\n",
    "#     return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "# xgboost_auto_tmp_1, xgboost_auto_tmp_2, xgboost_auto = xgboost_model(X, Y, cv, score)\n",
    "# print(xgboost_auto_tmp_1, xgboost_auto_tmp_2, xgboost_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be82f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'auto', 'n_neighbors': 2, 'p': 1, 'weights': 'distance'} 0.9328658251519402 KNeighborsClassifier(n_jobs=1, n_neighbors=2, p=1, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def KNeighborsClassifier_model(X, Y, cv, score):\n",
    "    KNeighborsClassifier_auto = KNeighborsClassifier(n_jobs=1)\n",
    "    param_dict = {\"n_neighbors\": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "                  \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                  \"weights\": ['uniform', 'distance'],\n",
    "                  \"p\": [1, 2],\n",
    "                  }\n",
    "    estimator = GridSearchCV(estimator=KNeighborsClassifier_auto, param_grid=param_dict, cv=cv, n_jobs=-1,\n",
    "                             scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto = KNeighborsClassifier_model(X, Y, cv, score)\n",
    "print(KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78954277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# def NearestNeighbors_model(X, Y, cv, score):\n",
    "#     # 首先实例化各个回归方法\n",
    "#     NearestNeighbors_auto = NearestNeighbors(n_jobs=1)\n",
    "#     param_dict = {\n",
    "#         #                 \"radius\":[0.5, 1, 1.5, 4],\n",
    "#         \"p\": [1, 2],\n",
    "#         \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#     }\n",
    "#     estimator = GridSearchCV(estimator=NearestNeighbors_auto, param_grid=param_dict, cv=cv, n_jobs=-1,\n",
    "#                              scoring=score)\n",
    "#     estimator.fit(X, Y)\n",
    "#     return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# NearestNeighbors_auto_tmp_1,  NearestNeighbors_auto_tmp_2, NearestNeighbors_auto = NearestNeighbors_model(X, Y, cv, score)\n",
    "# print(NearestNeighbors_auto_tmp_1,  NearestNeighbors_auto_tmp_2, NearestNeighbors_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6d86519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_prior': 'True'} 0.7184198223468912 BernoulliNB(fit_prior='True')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def BernoulliNB_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    BernoulliNB_auto = BernoulliNB()\n",
    "    param_dict = {\"fit_prior\": [\"True\", \"False\"],\n",
    "                  }\n",
    "    estimator = GridSearchCV(estimator=BernoulliNB_auto, param_grid=param_dict, cv=cv, n_jobs=-1, scoring=score)\n",
    "    estimator.fit(X, Y)\n",
    "    return estimator.best_params_, estimator.best_score_, estimator.best_estimator_\n",
    "\n",
    "\n",
    "BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto = BernoulliNB_model(X, Y, cv, score)\n",
    "print(BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cf0beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} 0.6276764843384759 GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def GaussianNB_model(X, Y, cv, score):\n",
    "    # 首先实例化各个回归方法\n",
    "    GaussianNB_auto = GaussianNB()\n",
    "    param_dict = {}\n",
    "    estimator = GridSearchCV(estimator = GaussianNB_auto, param_grid=param_dict, cv=cv,n_jobs=-1,scoring = score)\n",
    "    estimator.fit(X,Y)\n",
    "    return estimator.best_params_,estimator.best_score_,estimator.best_estimator_\n",
    "\n",
    "GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto = GaussianNB_model(X, Y, cv, score)\n",
    "print(GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5583b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.36397409439087\n"
     ]
    }
   ],
   "source": [
    "b = time()\n",
    "time_all = b -a \n",
    "print(time_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46826e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "a = time()\n",
    "def binary_ROC(X,Y,k,fig_name,score):\n",
    "    \"\"\"\n",
    "    关于名字：binary_ROC 是全部进行二分类的Roc图像绘制\n",
    "    data: dataframe\n",
    "    X：   X变量数据的名字\n",
    "    Y：   label变量的列名\n",
    "    cv:   表示需要的折叠次数\n",
    "    :return:   fig 返回ROC图\n",
    "    \"\"\"\n",
    "    # 初始化分层搜索\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "    # 调用上文的自动机器学习模型\n",
    "    LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2, LogisticRegressionCV_auto = LogisticRegressionCV_mdoel(\n",
    "        X, Y, cv, score)\n",
    "    print(LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2,\n",
    "          LogisticRegressionCV_auto)\n",
    "    SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto = SGDClassifier_model(X, Y, cv, score)\n",
    "    print(SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto)\n",
    "    LinearDiscriminantAnalysis_auto_tmp_1, LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto = LinearDiscriminantAnalysis_model(\n",
    "        X, Y, cv, score)\n",
    "    print(LinearDiscriminantAnalysis_auto_tmp_1,\n",
    "          LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto)\n",
    "    SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto = SVC_model(X, Y, cv, score)\n",
    "    print(SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto)\n",
    "    DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2, DecisionTreeClassifier_auto = DecisionTreeClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2,\n",
    "          DecisionTreeClassifier_auto)\n",
    "    AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto = AdaBoostClassifier_model(X,\n",
    "                                                                                                                     Y,\n",
    "                                                                                                                     cv,\n",
    "                                                                                                                     score)\n",
    "    print(AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto)\n",
    "    BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2, BaggingClassifier_auto = BaggingClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2,\n",
    "          BaggingClassifier_auto)\n",
    "    GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto = GradientBoostingClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto)\n",
    "    RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2, RandomForestClassifier_auto = RandomForestClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2,\n",
    "          RandomForestClassifier_auto)\n",
    "    KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto = KNeighborsClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto)\n",
    "    BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto = BernoulliNB_model(X, Y, cv, score)\n",
    "    print(BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto)\n",
    "    GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto = GaussianNB_model(X, Y, cv, score)\n",
    "    print(GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto)\n",
    "\n",
    "    colors = [\n",
    "        '#00429d', '#2653a5', '#3a64ad', '#4b75b4', '#5b87bc', '#6b99c3',\n",
    "        '#7bacca', '#8cbed0', '#9fd0d6', '#b5e2dc', '#d1f3e0', '#ffffe0'\n",
    "    ]\n",
    "    #     创建model字典，其中的最佳模型就是取自上文的model\n",
    "    algorithm_models = [\n",
    "        LogisticRegressionCV_auto,\n",
    "        SGDClassifier_auto,\n",
    "        LinearDiscriminantAnalysis_auto,\n",
    "        SVC_auto,\n",
    "        DecisionTreeClassifier_auto,\n",
    "        AdaBoostClassifier_auto,\n",
    "        BaggingClassifier_auto,\n",
    "        GradientBoostingClassifier_auto,\n",
    "        RandomForestClassifier_auto,\n",
    "        KNeighborsClassifier_auto,\n",
    "        GaussianNB_auto,\n",
    "        BernoulliNB_auto,\n",
    "    ]\n",
    "\n",
    "    #     创建名字列表\n",
    "    algorithm_names = [\n",
    "        \"LogisticRegression\",\n",
    "        \"SGDClassifier\",\n",
    "        \"LinearDA\",\n",
    "        \"SVC\",\n",
    "        \"DecisionTree\",\n",
    "        \"AdaBoost\",\n",
    "        \"Bagging\",\n",
    "        \"GTBoost\",\n",
    "        \"RandomForest\",\n",
    "        \"KNeighbors\",\n",
    "        \"BernoulliNB\",\n",
    "        \"GaussianNB\"\n",
    "    ]\n",
    "\n",
    "    X_train_, X_test_, y_train, y_test = model_selection.train_test_split(\n",
    "        X, Y,\n",
    "        test_size=0.2,\n",
    "        random_state=0,\n",
    "        stratify=Y,\n",
    "        shuffle=True              # y_imbalance 是 label的那一列\n",
    "    )\n",
    "    print(y_test)\n",
    "\n",
    "    transfer = StandardScaler()\n",
    "    X_train_0 = transfer.fit_transform(X_train_)\n",
    "    X_test_0 = transfer.transform(X_test_)\n",
    "\n",
    "\n",
    "    # 下面开始绘图\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "    for (name, method, colorname) in zip(algorithm_names, algorithm_models,colors):\n",
    "        method.fit(X_train_0, y_train)\n",
    "        y_test_predprob = method.predict_proba(X_test_0)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
    "\n",
    "        plt.plot(fpr,\n",
    "                 tpr,\n",
    "                 lw=5,\n",
    "                 label='{} (AUC={:.3f})'.format(name, auc(fpr, tpr)),\n",
    "                 color=colorname)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', lw=5, color='grey')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=15)\n",
    "    plt.ylabel('True Positive Rate', fontsize=15)\n",
    "    plt.title('ROC Curve of Auto_ML ', fontsize=20)\n",
    "\n",
    "    if fig_name == False:\n",
    "        fig_name = \"ROC\"\n",
    "\n",
    "    plt.legend(loc='lower right',\n",
    "               fontsize=10,\n",
    "               title=fig_name,\n",
    "               shadow=True,\n",
    "               fancybox=True,\n",
    "               ncol=2)\n",
    "\n",
    "    ## 添加局部放大图\n",
    "    inset_ax = fig.add_axes([0.57, 0.40, 0.3, 0.3],facecolor=\"white\")  # 这是局部放大图四个角的位置\n",
    "    for (name, method, colorname) in zip(algorithm_names, algorithm_models,colors):\n",
    "        method.fit(X_train_0, y_train)\n",
    "        y_test_predprob = method.predict_proba(X_test_0)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
    "        inset_ax.plot(fpr,\n",
    "                      tpr,\n",
    "                      lw=5,\n",
    "                      label='{} (AUC={:.3f})'.format(name, auc(fpr, tpr)),\n",
    "                      color=colorname)\n",
    "        inset_ax.set_xlim([-0.01, 0.3])\n",
    "        inset_ax.set_ylim([0.7, 1.01])\n",
    "        inset_ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "tmp_a = binary_ROC(X,Y,5,\"111\",\"accuracy\")\n",
    "b = time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe39543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_model(X, Y, k,score):\n",
    "    \"\"\"\n",
    "    说明:\n",
    "        X ,Y :DataFrame 形式的数据\n",
    "        k 是几折分层验证的参数\n",
    "    :return Auc_data, Acc_data, Recall_data, Precision_data 四个评判指标的Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "    # 调用上文的自动机器学习模型\n",
    "    LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2, LogisticRegressionCV_auto = LogisticRegressionCV_mdoel(\n",
    "        X, Y, cv, score)\n",
    "    print(LogisticRegressionCV_auto_tmp_1, LogisticRegressionCV_auto_tmp_2,\n",
    "          LogisticRegressionCV_auto)\n",
    "    SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto = SGDClassifier_model(X, Y, cv, score)\n",
    "    print(SGDClassifier_auto_tmp_1, SGDClassifier_auto_tmp_2, SGDClassifier_auto)\n",
    "    LinearDiscriminantAnalysis_auto_tmp_1, LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto = LinearDiscriminantAnalysis_model(\n",
    "        X, Y, cv, score)\n",
    "    print(LinearDiscriminantAnalysis_auto_tmp_1,\n",
    "          LinearDiscriminantAnalysis_auto_tmp_2, LinearDiscriminantAnalysis_auto)\n",
    "    SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto = SVC_model(X, Y, cv, score)\n",
    "    print(SVC_auto_tmp_1, SVC_auto_tmp_2, SVC_auto)\n",
    "    DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2, DecisionTreeClassifier_auto = DecisionTreeClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(DecisionTreeClassifier_auto_tmp_1, DecisionTreeClassifier_auto_tmp_2,\n",
    "          DecisionTreeClassifier_auto)\n",
    "    AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto = AdaBoostClassifier_model(X,\n",
    "                                                                                                                     Y,\n",
    "                                                                                                                     cv,\n",
    "                                                                                                                     score)\n",
    "    print(AdaBoostClassifier_auto_tmp_1, AdaBoostClassifier_auto_tmp_2, AdaBoostClassifier_auto)\n",
    "    BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2, BaggingClassifier_auto = BaggingClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(BaggingClassifier_auto_tmp_1, BaggingClassifier_auto_tmp_2,\n",
    "          BaggingClassifier_auto)\n",
    "    GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto = GradientBoostingClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(GradientBoostingClassifier_auto_tmp_1, GradientBoostingClassifier_auto_tmp_2, GradientBoostingClassifier_auto)\n",
    "    RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2, RandomForestClassifier_auto = RandomForestClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(RandomForestClassifier_auto_tmp_1, RandomForestClassifier_auto_tmp_2,\n",
    "          RandomForestClassifier_auto)\n",
    "    KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto = KNeighborsClassifier_model(\n",
    "        X, Y, cv, score)\n",
    "    print(KNeighborsClassifier_auto_tmp_1, KNeighborsClassifier_auto_tmp_2, KNeighborsClassifier_auto)\n",
    "    BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto = BernoulliNB_model(X, Y, cv, score)\n",
    "    print(BernoulliNB_auto_tmp_1, BernoulliNB_auto_tmp_2, BernoulliNB_auto)\n",
    "    GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto = GaussianNB_model(X, Y, cv, score)\n",
    "    print(GaussianNB_auto_tmp_1, GaussianNB_auto_tmp_2, GaussianNB_auto)\n",
    "\n",
    "    colors = [\n",
    "        '#00429d', '#2653a5', '#3a64ad', '#4b75b4', '#5b87bc', '#6b99c3',\n",
    "        '#7bacca', '#8cbed0', '#9fd0d6', '#b5e2dc', '#d1f3e0', '#ffffe0'\n",
    "    ]\n",
    "    #     创建model字典，其中的最佳模型就是取自上文的model\n",
    "    algorithm_models = [\n",
    "        LogisticRegressionCV_auto,\n",
    "        SGDClassifier_auto,\n",
    "        LinearDiscriminantAnalysis_auto,\n",
    "        SVC_auto,\n",
    "        DecisionTreeClassifier_auto,\n",
    "        AdaBoostClassifier_auto,\n",
    "        BaggingClassifier_auto,\n",
    "        GradientBoostingClassifier_auto,\n",
    "        RandomForestClassifier_auto,\n",
    "        KNeighborsClassifier_auto,\n",
    "        GaussianNB_auto,\n",
    "        BernoulliNB_auto,\n",
    "    ]\n",
    "\n",
    "    #     创建名字列表\n",
    "    algorithm_names = [\n",
    "        \"LogisticRegression\",\n",
    "        \"SGDClassifier\",\n",
    "        \"LinearDA\",\n",
    "        \"SVC\",\n",
    "        \"DecisionTree\",\n",
    "        \"AdaBoost\",\n",
    "        \"Bagging\",\n",
    "        \"GTBoost\",\n",
    "        \"RandomForest\",\n",
    "        \"KNeighbors\",\n",
    "        \"BernoulliNB\",\n",
    "        \"GaussianNB\"\n",
    "    ]\n",
    "\n",
    "    # 指定存储结果的字典表 行是模型名字，列是次数\n",
    "    tmp_1 = np.zeros((len(algorithm_names), 5))\n",
    "    Auc_data = pd.DataFrame(tmp_1, index=algorithm_names, columns=range(5))\n",
    "    tmp_2 = np.zeros((len(algorithm_names), 5))\n",
    "    Acc_data = pd.DataFrame(tmp_2, index=algorithm_names, columns=range(5))\n",
    "    tmp_3 = np.zeros((len(algorithm_names), 5))\n",
    "    Recall_data = pd.DataFrame(tmp_3, index=algorithm_names, columns=range(5))\n",
    "    tmp_4 = np.zeros((len(algorithm_names), 5))\n",
    "    Precision_data = pd.DataFrame(tmp_4, index=algorithm_names, columns=range(5))\n",
    "\n",
    "    score_funcs = ['accuracy', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "    for (name, method) in zip(algorithm_names, algorithm_models):\n",
    "        #         print(name)\n",
    "        scores = cross_validate(method,\n",
    "                                X,\n",
    "                                Y,\n",
    "                                scoring=score_funcs,\n",
    "                                cv=cv,  # 直接嫁接 上面定义的 cv\n",
    "                                return_estimator=True)\n",
    "\n",
    "        model_auc = scores['test_accuracy']\n",
    "        model_acc = scores['test_roc_auc']\n",
    "        model_recall = scores['test_recall']\n",
    "        model_precision = scores['test_precision']\n",
    "\n",
    "        for i in range(5):\n",
    "            Auc_data[i].loc[name] = model_auc[i]\n",
    "            Acc_data[i].loc[name] = model_acc[i]\n",
    "            Recall_data[i].loc[name] = model_recall[i]\n",
    "            Precision_data[i].loc[name] = model_precision[i]\n",
    "\n",
    "    return Auc_data, Acc_data, Recall_data, Precision_data\n",
    "\n",
    "a = time()\n",
    "Auc_data, Acc_data, Recall_data, Precision_data = auto_model(X,Y,5,\"accuracy\")\n",
    "b = time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6766a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_violion(df1,df2,fig_name):\n",
    "    \"\"\"\n",
    "    :param df1: 小提琴左侧的函数\n",
    "    :param df2: 小提琴右侧的函数\n",
    "    :return:  小提琴图\n",
    "    \"\"\"\n",
    "    colors = [\n",
    "        '#00429d', '#2653a5', '#3a64ad', '#4b75b4', '#5b87bc', '#6b99c3',\n",
    "        '#7bacca', '#8cbed0', '#9fd0d6', '#b5e2dc', '#d1f3e0', '#ffffe0'\n",
    "    ]\n",
    "\n",
    "    # 处理A数据\n",
    "    aa = df1.T\n",
    "    aa.describe()\n",
    "    # 处理B数据\n",
    "    bb = df2.T\n",
    "    bb.describe()\n",
    "    # 处理合并数据\n",
    "    AA = aa.melt()\n",
    "    AA.columns = [\"Model_A\", \"ROC\"]\n",
    "    df_A = AA\n",
    "    BB = bb.melt()\n",
    "    BB.columns = [\"Model_B\", \"ACC\"]\n",
    "    df_B = BB\n",
    "    df_AB = pd.concat([df_A, df_B], axis=1)\n",
    "    df_AB_fig = df_AB.drop(\"Model_B\", axis=1)\n",
    "    df_AB_fig.columns = [\"Model\", \"ROC\", \"ACC\"]\n",
    "    df_AB_fig_1 = pd.melt(df_AB_fig, id_vars=\"Model\", value_name=\"Value\")\n",
    "\n",
    "    # 下面开始绘图\n",
    "    pal2 = sns.color_palette(['#6fa9ca', '#ea8c9c'])\n",
    "    sns.set_palette(pal2)\n",
    "    sns.palplot(pal2)\n",
    "    fig = plt.figure(figsize=(20, 10), dpi=300)\n",
    "    sns.violinplot(x=\"Model\",\n",
    "                   y=\"Value\",\n",
    "                   hue=\"variable\",\n",
    "                   data=df_AB_fig_1,\n",
    "                   split=True, inner=\"quartile\",\n",
    "                   )\n",
    "    # Decoration\n",
    "\n",
    "    plt.title(fig_name, fontsize=22)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize=12)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=17)  # get函数得到原来默认的参数\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=17)\n",
    "    ax.legend(fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "a = time()\n",
    "tmp = estimator_violion(Auc_data, Acc_data,\"accuracy\")\n",
    "b = time()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ba6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
